{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (0.17.2)\n",
      "Requirement already satisfied: filelock in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/tristanbrigham/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# !apt-get install build-essential python3-dev\n",
    "# !apt-get install cmake\n",
    "# !pip install XFoil\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for generating airfoil designs using online airfoil datasets\n",
    "# code is inspired by:\n",
    "# The Department of Energy [https://catalog.data.gov/dataset/airfoil-computational-fluid-dynamics-2k-shapes-25-aoas-3-re-numbers]\n",
    "\n",
    "\n",
    "\n",
    "# specify which platform we are running on \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import sklearn\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# for the GAN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# importing xfoil itself\n",
    "# from xfoil import XFoil\n",
    "# from xfoil.model import Airfoil\n",
    "\n",
    "\n",
    "\n",
    "# state variables\n",
    "MAC_PLATFORM = 1\n",
    "LINUX_PLATFORM = 2\n",
    "UNKNOWN_PLATFORM = 3\n",
    "PLATFORM = UNKNOWN_PLATFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the platform\n",
    "os_type = platform.system()\n",
    "if os_type == \"Darwin\":\n",
    "\tPLATFORM = MAC_PLATFORM\n",
    "elif os_type == \"Linux\":\n",
    "\tPLATFORM = LINUX_PLATFORM\n",
    "else:\n",
    "\tPLATFORM = UNKNOWN_PLATFORM\n",
    "\n",
    "# should we be verbose\n",
    "VERBOSE = False\n",
    "\n",
    "# getting the file that we are currently working in \n",
    "# CURR_FILE_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "CURR_FILE_DIR = \".\"\n",
    "\n",
    "# define the directory that we are working in for the example files\n",
    "EXAMPLE_DIR = \"Example Data\"\n",
    "\n",
    "# setting the path of the airfoil\n",
    "full_data_path = os.path.join(CURR_FILE_DIR, \"airfoil_2k_data.h5\")\n",
    "\n",
    "# path to the example data\n",
    "example_data_path = os.path.join(CURR_FILE_DIR, EXAMPLE_DIR, \"b737a.dat\")\n",
    "\n",
    "# defining the data files that we are gonna use\n",
    "output_file = \"polar.dat\"\n",
    "example_data_output = os.path.join(CURR_FILE_DIR, EXAMPLE_DIR, output_file)\n",
    "dump_file = \"dump.dat\"\n",
    "example_data_dump = os.path.join(CURR_FILE_DIR, EXAMPLE_DIR, dump_file)\n",
    "\n",
    "\n",
    "# the number of data points that describe each of the point clouds\n",
    "NUM_POINTS_POINT_CLOUD = 100\n",
    "\n",
    "\n",
    "# define the xfoil command depending on the platform\n",
    "## CHANGE THIS IF NEEDED\n",
    "if PLATFORM == MAC_PLATFORM:\n",
    "\thome_directory = os.path.expanduser(\"~\")\n",
    "\tXFOIL_COMMAND = f\"{home_directory}/Desktop/Xfoil-for-Mac/bin/xfoil\"\n",
    "elif PLATFORM == LINUX_PLATFORM:\n",
    "\tXFOIL_COMMAND = \"xfoil\"\n",
    "else:\n",
    "\traise NotImplementedError(\"Running program on unsupported platform.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now go ahead and define the gan that we are going to be training\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout_p=0.2):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(128, 100),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            # nn.Linear(101, 256),  \n",
    "        )\n",
    "\n",
    "    def forward(self, noise, c_lift):\n",
    "        x = torch.cat((noise, c_lift.unsqueeze(1)), 1)  \n",
    "        return self.fc(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # defining the sequential discriminator\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(101, 1024),  # 100 coordinates + 1 lift coefficient\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, points, c_lift):\n",
    "        x = torch.cat((points, c_lift.unsqueeze(1)), 1)\n",
    "        return self.fc(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for initializing the weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# create the models\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# initialize the weights \n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "# create the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# create optimizers for the generator and discriminator\n",
    "optimizerG = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerD = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# function to create noise\n",
    "def generate_noise(size):\n",
    "    return torch.randn(size, NUM_POINTS_POINT_CLOUD)  \n",
    "\n",
    "# training loop\n",
    "def train_GAN(epochs, batch_size, data_loader):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "\n",
    "            # update D: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # getting the data and info about the data\n",
    "            real_data, labels = data\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # getting random lift values for the batch\n",
    "            real_c_lift = torch.FloatTensor(batch_size, 1).uniform_(0, 1)  \n",
    "\n",
    "            # run the discriminator\n",
    "            output = discriminator(real_data, real_c_lift).view(-1)\n",
    "\n",
    "            # retrieving error\n",
    "            errD_real = criterion(output, torch.ones(batch_size))\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # generating noise for the generator\n",
    "            noise = generate_noise(batch_size)\n",
    "            fake_data = generator(noise, real_c_lift)\n",
    "\n",
    "            # discriminator output\n",
    "            output = discriminator(fake_data.detach(), real_c_lift).view(-1)\n",
    "\n",
    "            # error propagation\n",
    "            errD_fake = criterion(output, torch.zeros(batch_size))\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            # update generator: maximize log(D(G(z)))\n",
    "            generator.zero_grad()\n",
    "            output = discriminator(fake_data, real_c_lift).view(-1)\n",
    "            errG = criterion(output, torch.ones(batch_size))\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # update us with the epochs\n",
    "            if i % 5 == 0:\n",
    "                print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "                      % (epoch, epochs, i, len(data_loader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d2/lsvbp7p92gnc5ql59sc72rx40000gn/T/ipykernel_68074/625747511.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(airfoil_data, dtype=torch.float32), torch.tensor(lift_values, dtype=torch.float32))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# creating the datasets\u001b[39;00m\n\u001b[1;32m     93\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mtensor(airfoil_data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), torch\u001b[38;5;241m.\u001b[39mtensor(lift_values, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m---> 94\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m train_GAN(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, data_loader\u001b[38;5;241m=\u001b[39mdataloader)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/torch/utils/data/sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# getting the points from a file path\n",
    "def get_points_from_dat_file(file_path):\n",
    "\n",
    "\t# open the file\n",
    "\twith open(file_path, 'r') as file:\n",
    "\t\tlines = file.readlines()\n",
    "\n",
    "\t# filter out non-numeric lines and strip whitespace\n",
    "\tpoints = []\n",
    "\tfor line in lines:\n",
    "\t\tparts = line.strip().split()\n",
    "\t\tif len(parts) == 2:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tpoints.append((float(parts[0]), float(parts[1])))\n",
    "\t\t\texcept ValueError:\n",
    "\t\t\t\t# print(f\"Skipping invalid line: {line}\")\n",
    "\t\t\t\tpass\n",
    "\n",
    "\treturn points\n",
    "\n",
    "# pull data tensor and information\n",
    "def get_folder_data(folder_path = None, file_name = None):\n",
    "\n",
    "\t# pull the data tensor\n",
    "\tpoints = get_points_from_dat_file(os.path.join(folder_path, file_name + \"_reformatted.dat\"))\n",
    "\t\t\n",
    "\t# get the tensor\n",
    "\tpoints = torch.Tensor(points)\n",
    "\n",
    "\t# get the other parameters that we want\n",
    "\tresulting_data = pd.read_csv(os.path.join(folder_path, \"polar.dat\"), delim_whitespace=True, skiprows=10)\n",
    "\tresulting_data = resulting_data.drop(0)\n",
    "\n",
    "\t# pulling individual values\n",
    "\talpha = resulting_data[\"alpha\"].loc[0]\n",
    "\tCL = resulting_data['CL'].loc[0]         \n",
    "\tCD = resulting_data['CD'].loc[0]        \n",
    "\tCDp = resulting_data['CDp'].loc[0]         \n",
    "\tCM = resulting_data['CM'].loc[0]  \n",
    "\n",
    "\t# create labels tensor\n",
    "\tlabels = torch.Tensor([CL, CDp, CM])\n",
    "\n",
    "\t# returning the generated values\n",
    "\treturn (points, labels)\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\n",
    "# define a function to create the tensors that we need\n",
    "def pull_airfoil_data(wd=EXAMPLE_DIR):\n",
    "\n",
    "\t# the lists we will return\n",
    "\tret_data_tensor = []\n",
    "\tret_labels_tensor = []\n",
    "\n",
    "\t# iterate through all of the potential airfoils\n",
    "\tfor airfoil_dat in os.listdir(wd):\n",
    "\n",
    "\t\t# print the airfoil type\n",
    "\t\tdata_path = os.path.join(airfoil_dat)\n",
    "\n",
    "\t\t# pull the data\n",
    "\t\ttry:\n",
    "\t\t\tt_data, t_labels = get_folder_data(folder_path=data_path, file_name=airfoil_dat)\n",
    "\t\texcept:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# add the data to the list\n",
    "\t\tret_data_tensor.append(t_data)\n",
    "\t\tret_labels_tensor.append(t_labels)\n",
    "\n",
    "\t# change the final versions to tensors\n",
    "\tret_data_tensor = torch.Tensor(ret_data_tensor)\n",
    "\tret_labels_tensor = torch.Tensor(ret_labels_tensor)\n",
    "\n",
    "\treturn ret_data_tensor, ret_labels_tensor\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# the data sets are going to be formed of:\n",
    "# 100-dimensional vector representing the input airfoil\n",
    "\n",
    "# 3-dimensional vector representing the lift coefficient, drag-pressure coefficient, and moment coefficient\n",
    "\n",
    "airfoil_data, lift_values = pull_airfoil_data(wd=EXAMPLE_DIR)\n",
    "\n",
    "# creating the datasets\n",
    "dataset = TensorDataset(torch.tensor(airfoil_data, dtype=torch.float32), torch.tensor(lift_values, dtype=torch.float32))\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_GAN(epochs=50, batch_size=32, data_loader=dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc552",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
